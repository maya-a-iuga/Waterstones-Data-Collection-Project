{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zh/9q0t4zld60d2qm1_23q36qwh0000gn/T/ipykernel_22642/1285710457.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "import argparse\n",
    "from collections import Iterable\n",
    "import uuid\n",
    "import os\n",
    "import json\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper():\n",
    "\n",
    "    #use selenium to open desired webpage using Chrome\n",
    "    def __init__(self, url):\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.url = url\n",
    "        self.page = self.driver.get(self.url)\n",
    "\n",
    "        #main book categories of the website\n",
    "        self.book_main_categories = []\n",
    "        self.category_subcategories = []\n",
    "        self.books_list_page = []\n",
    "        self.books_list_slider = []\n",
    "\n",
    "        #create empty metadata dictionary\n",
    "        self.metadata_dictionary = {\"Unique id\" : \" \",\n",
    "                                    \"UUID\" : \" \",\n",
    "                                    \"Book Title\" : \" \",\n",
    "                                    \"Author\" : \" \",\n",
    "                                    \"Initial Price\" : \" \",\n",
    "                                    \"Current Price\" : \" \",\n",
    "                                    \"ISBN\" : \" \",\n",
    "                                    \"Number of Pages\" : \" \",\n",
    "                                    \"Published Date\" : \" \",\n",
    "                                    \"Publisher\" : \" \",\n",
    "                                    \"Stock\" : \" \",\n",
    "                                    \"Availability\" : \" \",\n",
    "                                    \"Height\" : \" \",\n",
    "                                    \"Width\" : \" \",\n",
    "                                    \"Link to image\": \" \",\n",
    "                                    \"Bookstore Name\" : \" \",\n",
    "                                    \"Bookstore Address\" : \" \",\n",
    "                                    \"Schedule\" : \" \",\n",
    "                                    \"Collection Time\" : \" \"}\n",
    "\n",
    "        self.list_dict_metadata = []\n",
    "\n",
    "    #bypass cookies\n",
    "    def bypass_cookies(self):\n",
    "    \n",
    "        # wait so website doesn't suspect you are a bot\n",
    "        time.sleep(2)\n",
    "\n",
    "        # if website has cookies\n",
    "        try:\n",
    "            accept_cookies_button = self.driver.find_element_by_xpath('//*[@id=\"onetrust-accept-btn-handler\"]')\n",
    "            accept_cookies_button.click()\n",
    "\n",
    "        # if website doesn't have cookies \n",
    "        except: \n",
    "            pass\n",
    "\n",
    "    # define flags for different conditions under which you can run the scraper\n",
    "    def scraper_flags(self):\n",
    "        # first flag is for choosing desired book category\n",
    "        self.category_parser = argparse.ArgumentParser(description = \"Which main book category you want to select?\")\n",
    "\n",
    "        ##for when runing .py file!!\n",
    "        # f for fiction\n",
    "        self.category_parser.add_argument('-f', action = \"store_true\")\n",
    "        # c for crime\n",
    "        self.category_parser.add_argument('-c', action = \"store_true\")\n",
    "        # sf for science-fiction\n",
    "        self.category_parser.add_argument('-sf', action = \"store_true\")\n",
    "        # g for graphic novels and manga\n",
    "        self.category_parser.add_argument('-g', action = \"store_true\")\n",
    "        # nf for non-fiction\n",
    "        self.category_parser.add_argument('-nf', action = \"store_true\")\n",
    "\n",
    "        ## for now in jupyter notebook use the below\n",
    "        #self.category_parser.add_argument(category_flag, action = \"store_true\")\n",
    "\n",
    "        self.category_args = self.category_parser.parse_args()\n",
    "\n",
    "    \n",
    "    #click/access one of the links\n",
    "    def access_book_category(self, item):\n",
    "        self.driver.get(item)  \n",
    "\n",
    "    #retrieves list of link with main books categories\n",
    "    def get_main_books_categories(self, desired_category):\n",
    "\n",
    "        #first part of xpath always navigated to desktop version of website desktop-navs\n",
    "        book_category_path = self.driver.find_element_by_xpath('//div[@class = \"navs-container desktop-navs\"]/ul[@class = \"subnavs\"][1]/li[2]')\n",
    "        # get all a elements containing the book categories\n",
    "        books_main_list = book_category_path.find_elements_by_xpath('.//span[@class = \"name nav-header-link\"]/a')\n",
    "        # we only interested in 5 categories: fiction, crime, science finction, graphic novel and non-fiction\n",
    "        # leaves out the children book categories\n",
    "        books_main_list = books_main_list[0:5]\n",
    "\n",
    "        # make this into a list compression \n",
    "        for item in books_main_list:\n",
    "            #a_tag = item.find_element_by_tag_name('a')\n",
    "            link = item.get_attribute('href')\n",
    "            self.book_main_categories.append(link)\n",
    "\n",
    "        #return book_main_categories\n",
    "        #based on passed category_flag, return desired link for next method\n",
    "        ## FOR WHEN RUNING .PY SCRIPT\n",
    "        #if self.category_args.fiction == True:\n",
    "         #   return self.book_main_categories[0]\n",
    "        #elif self.category_args.c == True:\n",
    "         #   return self.book_main_categories[1]\n",
    "        #elif self.category_args.sf == True:\n",
    "         #   return self.book_main_categories[2]\n",
    "        #elif self.category_args.g == True:\n",
    "         #   return self.book_main_categories[3]\n",
    "        #elif self.category_args.nf == True:\n",
    "         #   return self.book_main_categories[4]\n",
    "\n",
    "         ## JUST FOR NOW FOR IPYNB FILES\n",
    "        if desired_category == 'fiction':\n",
    "            return self.book_main_categories[0]\n",
    "        elif desired_category == 'crime':\n",
    "            return self.book_main_categories[1]\n",
    "        elif desired_category == 'science fiction':\n",
    "            return self.book_main_categories[2]\n",
    "        elif desired_category == 'graphic novel':\n",
    "            return self.book_main_categories[3]\n",
    "        elif desired_category == 'non fiction':\n",
    "            return self.book_main_categories[4]\n",
    "\n",
    "\n",
    "\n",
    "    #from main category - can now access all sub-categories buttons\n",
    "    def get_category_subcategories(self, desired_category):\n",
    "\n",
    "        self.category_subcategories = []\n",
    "        # calls function to retrieve desired book category and then loads corresponding page\n",
    "        category_header = self.get_main_books_categories(desired_category)\n",
    "        self.access_book_category(category_header)\n",
    "\n",
    "        #find list of category sub-divisions\n",
    "        subcategories_list = self.driver.find_elements_by_xpath('//div[@class = \"span3 tablet-span6 mobile-span6\"]//a')\n",
    "\n",
    "        for item in subcategories_list:\n",
    "            link = item.get_attribute('href')\n",
    "            self.category_subcategories.append(link)\n",
    "\n",
    "    \n",
    "    ## subcategory page: check if 'Our best <subcategory> exist if so you can scroll left/right and press\n",
    "    ## see all to display all the pages, otherwise skips this steps and directly goes to all pages display\n",
    "    \n",
    "    def finds_best_subcategory_list(self):\n",
    "\n",
    "        #for subcategory in self.category_subcategories :\n",
    "\n",
    "        # if header and see more button then press this - get data from full pages\n",
    "        \n",
    "        header_path = self.driver.find_element_by_xpath('//header[@class = \"span12 pages-header-row\"]')\n",
    "\n",
    "            #find see all button - sometimes called see more but same xpath\n",
    "        self.see_all_button = header_path.find_element_by_xpath('./a[@class = \"button button-teal\"]')\n",
    "        self.see_all_button.click()\n",
    "        time.sleep(3)\n",
    "\n",
    "        try:\n",
    "            header_path = self.driver.find_element_by_xpath('//header[@class = \"span12 pages-header-row\"]')\n",
    "            self.see_all_button = header_path.find_element_by_xpath('./a[@class = \"button button-teal\"]')\n",
    "            self.see_all_button.click()\n",
    "            time.sleep(3)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        #self.get_book_list_page(3)\n",
    "        \n",
    "        #if you want instead to only get information from slider\n",
    "        #self.get_book_list_slider()\n",
    "                        \n",
    "        \n",
    "    ##GET FINAL BOOKS LIST\n",
    "    \n",
    "    #navigation method 1: get current url, scroll down, get next page url - FASTEST METHOD\n",
    "    #returns book list from see more pages\n",
    "    def get_book_list_page(self, pages_no):\n",
    "        \n",
    "        self.books_list_page = []\n",
    "\n",
    "        if self.driver.current_url[-7 : -1] == \"?page=\":\n",
    "\n",
    "            self.current_url = self.driver.current_url[:-7] + '/page/'\n",
    "\n",
    "            i = 1\n",
    "            while i <= pages_no:\n",
    "\n",
    "                self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                list_books_links = self.driver.find_elements_by_xpath('//div[@class = \"image-wrap\"]/a')\n",
    "                for item in list_books_links:\n",
    "                    link = item.get_attribute('href')\n",
    "                    self.books_list_page.append(link)\n",
    "\n",
    "                i += 1\n",
    "                self_new_current_url = ''\n",
    "                self.new_current_url = self.current_url + str(i)\n",
    "                self.driver.get(self.new_current_url) \n",
    "\n",
    "        \n",
    "        elif self.driver.current_url[-7 : -1] == \"/page/\":\n",
    "\n",
    "            self.current_url = self.driver.current_url[:-1]\n",
    "\n",
    "            i = 1\n",
    "            while i <= pages_no:\n",
    "\n",
    "                self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                list_books_links = self.driver.find_elements_by_xpath('//div[@class = \"image-wrap\"]/a')\n",
    "                for item in list_books_links:\n",
    "                    link = item.get_attribute('href')\n",
    "                    self.books_list_page.append(link)\n",
    "\n",
    "                i += 1\n",
    "                self_new_current_url = ''\n",
    "                self.new_current_url = self.current_url + str(i)\n",
    "                self.driver.get(self.new_current_url)\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.current_url = self.driver.current_url + '/page/'\n",
    "            i = 1\n",
    "            while i <= pages_no:\n",
    "\n",
    "                self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                list_books_links = self.driver.find_elements_by_xpath('//div[@class = \"image-wrap\"]/a')\n",
    "                for item in list_books_links:\n",
    "                    link = item.get_attribute('href')\n",
    "                    self.books_list_page.append(link)\n",
    "\n",
    "                i += 1\n",
    "                self_new_current_url = ''\n",
    "                self.new_current_url = self.current_url + str(i)\n",
    "                self.driver.get(self.new_current_url)\n",
    "    \n",
    "    # Method to remove duplicate book links\n",
    "    def remove_duplicate_book_links(self, list):\n",
    "     for item in list:\n",
    "         if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "             for x in self.remove_duplicate_book_links(item):\n",
    "                 yield x\n",
    "         else:        \n",
    "             yield item\n",
    "    \n",
    "    #def check_key_exist(self, my_dict, my_key, my_value):\n",
    "     #   if my_key in my_dict:\n",
    "      #      if not isinstance(my_dict[my_key], list):\n",
    "       #         my_dict[my_key] = [my_dict[my_key]]\n",
    "        #    my_dict[my_key].append(my_value)\n",
    "        #else:\n",
    "         #   my_dict[my_key] = my_value\n",
    "    \n",
    "    #generate unique id for each book link page - is this id user friendly??/ could use ISBN of each book\n",
    "\n",
    "    def generate_unique_id(self, item):\n",
    "\n",
    "        self.metadata_dictionary[\"Unique id\"] = item.split('/')[-1]\n",
    "        \n",
    "    \n",
    "    ## loop through book links, access one at a time and then collect the meta-data from each book page\n",
    "    def collect_book_metadata(self, list):\n",
    "        \n",
    "        self.list_dict_metadata = []\n",
    "\n",
    "        for item in list:\n",
    "\n",
    "            # access individual book page\n",
    "            self.access_book_category(item)\n",
    "\n",
    "            #generate unique ids from page id and uuid\n",
    "            self.generate_unique_id(item)\n",
    "            self.metadata_dictionary[\"UUID\"] = str(uuid.uuid4())\n",
    "            \n",
    "            # get all the structured metadata\n",
    "\n",
    "            book_title = self.driver.find_element_by_xpath('//span[@class = \"book-title\"]').text\n",
    "            self.metadata_dictionary[\"Book Title\"] = book_title\n",
    "\n",
    "            author = self.driver.find_element_by_xpath('//span[@itemprop = \"author\"]').text\n",
    "            self.metadata_dictionary[\"Author\"] = author\n",
    "            \n",
    "            # for coming soon category \n",
    "            try:\n",
    "                initial_price = self.driver.find_element_by_xpath('//b[@class = \"price-rrp\"]').text\n",
    "                no_pages = self.driver.find_element_by_xpath('//span[@itemprop = \"numberOfPages\"]').text\n",
    "                stock = self.driver.find_element_by_xpath('//span[@id = \"scope_offer_availability\"]').text\n",
    "                availability = self.driver.find_element_by_xpath('//p[@class = \"stock-message\"]').text\n",
    "\n",
    "            except:\n",
    "                initial_price = 'NaN'\n",
    "                no_pages = 'NaN'\n",
    "                stock = 'NaN'\n",
    "\n",
    "            self.metadata_dictionary[\"Initial Price\"] = initial_price\n",
    "            self.metadata_dictionary[\"Number of Pages\"] = no_pages\n",
    "            self.metadata_dictionary[\"Stock\"] = stock\n",
    "            self.metadata_dictionary[\"Availability\"] = availability\n",
    "\n",
    "            current_price = self.driver.find_element_by_xpath('//b[@itemprop = \"price\"]').text\n",
    "            self.metadata_dictionary[\"Current Price\"] = current_price\n",
    "                \n",
    "            published_date = (self.driver.find_element_by_xpath('//meta[@itemprop = \"datePublished\"]')).get_attribute('content')\n",
    "            self.metadata_dictionary[\"Published Date\"] = published_date\n",
    "\n",
    "            publisher = self.driver.find_element_by_xpath('//p[@class = \"spec\"]/i[1]/span').get_attribute(\"innerHTML\")\n",
    "            self.metadata_dictionary[\"Publisher\"] = publisher\n",
    "\n",
    "            isbn = self.driver.find_element_by_xpath('//p[@class = \"spec\"]/i[2]/span').get_attribute(\"innerHTML\")\n",
    "            self.metadata_dictionary[\"ISBN\"] = isbn\n",
    "\n",
    "            #if dimensions etc try [4] /except [5]\n",
    "            if len(self.driver.find_elements_by_xpath('//p[@class = \"spec\"]/i')) == 6:\n",
    "                height = self.driver.find_element_by_xpath('//p[@class = \"spec\"]/i[3]/span/span[1]').get_attribute(\"innerHTML\")\n",
    "                width = self.driver.find_element_by_xpath('//p[@class = \"spec\"]/i[3]/span/span[2]').get_attribute(\"innerHTML\")    \n",
    "\n",
    "            elif len(self.driver.find_elements_by_xpath('//p[@class = \"spec\"]/i')) == 8:\n",
    "                height = self.driver.find_element_by_xpath('//p[@class = \"spec\"]/i[4]/span/span[1]').get_attribute(\"innerHTML\")\n",
    "                width = self.driver.find_element_by_xpath('//p[@class = \"spec\"]/i[4]/span/span[2]').get_attribute(\"innerHTML\")\n",
    "\n",
    "            elif len(self.driver.find_elements_by_xpath('//p[@class = \"spec\"]/i')) == 10:\n",
    "                # for normal case scenario\n",
    "                \n",
    "                if self.driver.find_element_by_xpath('//p[@class = \"spec\"]/i[5]').get_attribute(\"innerHTML\")[0:9] == 'Dimension':\n",
    "\n",
    "                    height = self.driver.find_element_by_xpath('//p[@class = \"spec\"]/i[5]/span/span[1]').get_attribute(\"innerHTML\")\n",
    "                    width = self.driver.find_element_by_xpath('//p[@class = \"spec\"]/i[5]/span/span[2]').get_attribute(\"innerHTML\")\n",
    "                  \n",
    "                elif self.driver.find_element_by_xpath('//p[@class = \"spec\"]/i[5]').get_attribute(\"innerHTML\")[0:7] == 'Edition':\n",
    "\n",
    "                    height = self.driver.find_element_by_xpath('//p[@class = \"spec\"]/i[4]/span/span[1]').get_attribute(\"innerHTML\")\n",
    "                    width = self.driver.find_element_by_xpath('//p[@class = \"spec\"]/i[4]/span/span[2]').get_attribute(\"innerHTML\")\n",
    "            \n",
    "            self.metadata_dictionary[\"Height\"] = height\n",
    "            self.metadata_dictionary[\"Width\"] = width\n",
    "            \n",
    "            # get the image links/unstructured data\n",
    "            image_links = self.driver.find_element_by_xpath('//div[@class = \"book-image-main\"]//img').get_attribute(\"src\")\n",
    "            self.metadata_dictionary[\"Link to image\"] = image_links\n",
    "\n",
    "            #find the click & collect metadata\n",
    "            try:\n",
    "                self.click_and_collect()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            time.sleep(4)\n",
    "            try:\n",
    "                bookstore_name = self.driver.find_element_by_xpath('//div[@class = \"store\"][1]//div[@class = \"title\"]').get_attribute(\"innerHTML\")\n",
    "\n",
    "                bookstore_address = self.driver.find_element_by_xpath('//div[@class = \"store\"][1]//div[@class = \"address\"]').get_attribute(\"innerHTML\")\n",
    "\n",
    "                bookstore_schedule = self.driver.find_element_by_xpath('//div[@class = \"store\"][1]//div[@class = \"hours\"]').get_attribute(\"innerHTML\")\n",
    "\n",
    "                collection_time = self.driver.find_element_by_xpath('//div[@class = \"store\"][1]//div[4]').get_attribute(\"innerHTML\")\n",
    "            \n",
    "            except NoSuchElementException:\n",
    "                bookstore_name = 'Book is not out yet'\n",
    "                bookstore_address = 'Book is not out yet'\n",
    "                bookstore_schedule = 'Book is not out yet'\n",
    "                collection_time = 'Book is not out yet'\n",
    "\n",
    "\n",
    "            self.metadata_dictionary[\"Bookstore Name\"] = bookstore_name\n",
    "            self.metadata_dictionary[\"Bookstore Address\"] = bookstore_address\n",
    "            self.metadata_dictionary[\"Collection Time\"] = collection_time\n",
    "            \n",
    "            self.metadata_dictionary[\"Schedule\"] = bookstore_schedule\n",
    "            #if book is not available for collection today\n",
    "            if self.metadata_dictionary[\"Schedule\"] == '':\n",
    "                self.metadata_dictionary[\"Schedule\"] = \"Book not available for collection today\"\n",
    "            \n",
    "\n",
    "        # now append dictionary to a list of dictionaries\n",
    "            dictionary_copy = self.metadata_dictionary.copy()\n",
    "            self.list_dict_metadata.append(dictionary_copy)       \n",
    "            \n",
    "    ## first method: enter book and navigate to click & collect information\n",
    "    # will need to adapt it to loop through all books\n",
    "    def click_and_collect(self):\n",
    "        #self.access_book_category(self.all_links_next_page, index)\n",
    "\n",
    "        click_and_collect_button = self.driver.find_element_by_xpath('//div[@class = \"book-actions\"]//button[@class = \"button button-gold js-open-modal\"]')\n",
    "        click_and_collect_button.click()\n",
    "\n",
    "        search_bar = self.driver.find_element_by_xpath('//input[@placeholder = \"Town, city, or postcode\"]')\n",
    "        time.sleep(2)\n",
    "        search_bar.send_keys(\"WC1 0RW\")\n",
    "\n",
    "        go_button = self.driver.find_element_by_xpath('//button[@id = \"searchterm\"]')\n",
    "        go_button.click()\n",
    "\n",
    "\n",
    "    #create folders for the data to be saved in\n",
    "    def create_raw_data_folder(self):\n",
    "\n",
    "        root_directory = '/Users/maya/Desktop/AiCore_git/Data_Collection_Project'\n",
    "        raw_data_directory =  os.path.join(root_directory, 'raw_data')\n",
    "\n",
    "        if root_directory == os.getcwd():\n",
    "\n",
    "            if os.path.exists(raw_data_directory):\n",
    "                os.chdir(raw_data_directory)\n",
    "            else:\n",
    "                os.mkdir('raw_data')\n",
    "                os.chdir(raw_data_directory)\n",
    "        else:\n",
    "            os.chdir(raw_data_directory)\n",
    "\n",
    "    def create_category_folders(self, category):\n",
    "        \n",
    "        self.create_raw_data_folder()\n",
    "        current_directory = os.getcwd()\n",
    "        category_directory = os.path.join(current_directory, category)\n",
    "\n",
    "        if current_directory:\n",
    "\n",
    "            if os.path.exists(category_directory):\n",
    "                os.chdir(category_directory)\n",
    "            else:\n",
    "                os.mkdir(category)\n",
    "                os.chdir(category_directory)\n",
    "        else:\n",
    "            os.chdir(category_directory)\n",
    "\n",
    "\n",
    "    def create_subcategory_folders(self, subcategory):\n",
    "\n",
    "        #self.create_category_folders(category)\n",
    "        current_directory = os.getcwd()\n",
    "        subcategory_directory = os.path.join(current_directory, subcategory)\n",
    "\n",
    "        if os.path.exists(subcategory_directory):\n",
    "            os.chdir(subcategory_directory)\n",
    "        else:\n",
    "            os.mkdir(subcategory)\n",
    "            os.chdir(subcategory_directory)\n",
    "        \n",
    "        self.saving_directory = subcategory_directory\n",
    "    \n",
    "    def save_book_covers(self):\n",
    "        for image in range(len(self.list_dict_metadata)):\n",
    "\n",
    "            image_url = self.list_dict_metadata[image][\"Link to image\"]\n",
    "            save_path = os.path.join(os.getcwd(), (str(image) + '.jpg'))\n",
    "            opener = urllib.request.build_opener()\n",
    "            opener.addheaders = [('User-Agent', 'MyApp/1.0')]\n",
    "            urllib.request.install_opener(opener)\n",
    "            urllib.request.urlretrieve(image_url, save_path)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### also removes duplicate links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Iterable\n",
    "def flatten(lis):\n",
    "     for item in lis:\n",
    "         if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "             for x in flatten(item):\n",
    "                 yield x\n",
    "         else:        \n",
    "             yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if name == main run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    waterstones = Scraper(\"https://www.waterstones.com\")\n",
    "    waterstones.bypass_cookies()\n",
    "\n",
    "    #list_categories = [\"fiction\", \"crime\", \"science fiction\", \"graphic novel\", \"non fiction\"]\n",
    "    list_categories = [\"fiction\"]\n",
    "    #final_book_list = []\n",
    "\n",
    "\n",
    "    for category in list_categories:\n",
    "        \n",
    "        #create folders for each category\n",
    "        waterstones.create_category_folders(category)\n",
    "\n",
    "        #returns list of sub-categories\n",
    "        waterstones.get_category_subcategories(category)\n",
    "\n",
    "        for subcategory in waterstones.category_subcategories[0:1]:\n",
    "            \n",
    "            #creates folder for current subcategory\n",
    "            subcategory_folder = subcategory.split('/')[-1]\n",
    "            waterstones.create_subcategory_folders(subcategory_folder)\n",
    "\n",
    "            # access individual page, scrolls through and retrieves links for books\n",
    "            final_book_list = []\n",
    "            waterstones.access_book_category(subcategory)\n",
    "            waterstones.finds_best_subcategory_list()\n",
    "            waterstones.get_book_list_page(1)\n",
    "            #final_book_list.append(waterstones.books_list_page)\n",
    "            final_book_list = waterstones.books_list_page\n",
    "            \n",
    "           # list without duplicate book links\n",
    "            #currated_list = []\n",
    "            currated_list = list(waterstones.remove_duplicate_book_links(final_book_list))\n",
    "\n",
    "           #collects all the metadta\n",
    "            waterstones.collect_book_metadata(currated_list)\n",
    "\n",
    "            # saves list of dictionary metadata as json file\n",
    "            with open(os.path.join(waterstones.saving_directory, 'data.json'), 'w') as folder:\n",
    "                json.dump(waterstones.list_dict_metadata, folder)\n",
    "\n",
    "            # create image folder to save all book covers for this sub-category\n",
    "            if os.path.exists(os.path.join(waterstones.saving_directory, 'images')):\n",
    "                os.chdir(os.path.join(waterstones.saving_directory, 'images'))\n",
    "            else:\n",
    "                os.mkdir('images')\n",
    "                os.chdir(os.path.join(waterstones.saving_directory, 'images'))\n",
    "\n",
    "            #save images\n",
    "            waterstones.save_book_covers()\n",
    "\n",
    "            # go back to category folder (first exist images subfolder then goes back)\n",
    "            os.chdir('../..')\n",
    "        \n",
    "        #go back to raw-data folder to create new category folder on next iteration\n",
    "        os.chdir('..')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extra code from initial scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2578035342.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [117]\u001b[0;36m\u001b[0m\n\u001b[0;31m    def navigation_method_1(self, pages_no):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "   #method 1: scroll down to end of page, wait, press load more and so on (number rep = number of pages scrolled)\n",
    "    #inside this method later will need to also get list of all links to the books -SECOND PLACE SPEED\n",
    "    def navigation_method_1(self, pages_no):\n",
    "\n",
    "        scroll_pause_time = 2\n",
    "        #gets current page height\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        #here using while true loop it go on forever - for now add counter to only load first 5 pages\n",
    "        #some problem with page number!! only loads until page 4 but no further?\n",
    "        i = 0\n",
    "        while i <= pages_no:\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "\n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            if new_height == last_height:\n",
    "\n",
    "                all_books = self.driver.find_elements_by_xpath('//div[@class = \"image-wrap\"]/a')\n",
    "                for item in all_books:\n",
    "                    link = item.get_attribute('href')\n",
    "                    self.all_links_page.append(link)\n",
    "                show_more_button = self.driver.find_element_by_xpath('//button[@class = \"button button-teal\"]')\n",
    "                show_more_button.click()\n",
    "                break\n",
    "\n",
    "            #once at the bottom of page and see more pressed- reinitialises height\n",
    "            last_height = new_height\n",
    "            i +=1\n",
    "    \n",
    "    #method 2: scroll down, scroll back up and then press next arrow to navigate to next page -SLOWEST\n",
    "    def navigation_method_2(self, pages_no):\n",
    "\n",
    "        scroll_pause_time = 2\n",
    "\n",
    "        i = 1\n",
    "        while i <= pages_no:\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            all_books_this_page = self.driver.find_elements_by_xpath('//div[@class = \"image-wrap\"]/a')\n",
    "            for item in all_books_this_page:\n",
    "                link = item.get_attribute('href')\n",
    "                self.all_links_next_page.append(link)\n",
    "\n",
    "            #time.sleep(scroll_pause_time)\n",
    "\n",
    "            try:\n",
    "                go_back_up_button = self.driver.find_element_by_xpath('//div[@class = \"backtotop show stick\"]')\n",
    "                go_back_up_button.click()\n",
    "            except:\n",
    "                go_back_up_button = self.driver.find_element_by_xpath('//div[@class = \"backtotop show\"]')\n",
    "                go_back_up_button.click()\n",
    "\n",
    "            time.sleep(scroll_pause_time)\n",
    "\n",
    "            next_page = self.driver.find_element_by_xpath('//a[@title = \"Go to next page\"]')\n",
    "            next_page.click()\n",
    "            i += 1\n",
    "            #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    #alternatively can get directly a link to all existent sub-categories of all main categories\n",
    "    def get_all_subcategories(self):\n",
    "        all_subcategories = self.driver.find_elements_by_xpath('//div[@class = \"navs-container desktop-navs\"]//li[@class = \"subnav\"][2]//li[@class = \"subnav-item\"]')\n",
    "\n",
    "        for item in all_subcategories:\n",
    "            a_tag = item.find_element_by_tag_name('a')\n",
    "            link = a_tag.get_attribute('href')\n",
    "            self.all_subcategories.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "    # method 2 returning book list from slider bar\n",
    "    def get_book_list_slider(self):\n",
    "\n",
    "        swiper_path = self.driver.find_element_by_xpath('//div[@class = \"swiper-wrapper\"]')\n",
    "        titles_path = swiper_path.find_elements_by_xpath('.//div[@class = \"title-wrap\"]')\n",
    "\n",
    "        for item in titles_path:\n",
    "            a_tag = item.find_element_by_tag_name('a')\n",
    "            link = a_tag.get_attribute('href')\n",
    "            self.books_list_slider.append(link)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "545e036c4b32438aced1f6b3c8d38ca151d9c36189e05839cb0aa568fda70ddd"
  },
  "kernelspec": {
   "display_name": "Python ('Data_Collection_Project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
